[annotation]
# 多个特征融合的命名实体识别
task_type= multi_feature_ner
[samples_generate]
# 存放ann和txt数据的路，默认ann和txt在同一个路径下
ann_dir=/nlp_data/qinye/raw_data_base/ann_process/
sample_save_path=/nlp_data/qinye/ner_task/data/raws/
log=/nlp_data/qinye/ner_task/log/samples_generate/

[dataset_split]
# 数据集切分比例  train/dev/test
split_ratio=0.8,0.2,0
# 数据集保存路径
dataset_save_path=/nlp_data/qinye/ner_task/dataset/multi_feature_ner/

[model_train]
#多gpu情况，指定使用的gpu编号  0,1 表示使用两个gpu
train_device_id=1
train_path=trainset
; dev_path=devset
dev_path=devset
test_path=testset
resume=0
checkpoint=/nlp_data/qinye/ner_task/checkpoint/multi_feature_ner/
history=/nlp_data/qinye/ner_task/history/multi_feature_ner
log=/nlp_data/qinye/ner_task/log/


model_choice=multi_feature
#模型名称
model_save_name=multi_feature_ner_v1
#读取模型名称（在resume=1时可以设置）
model_resume_name=/nlp_data/qinye/ner_task/checkpoint/multi_feature_ner

bert_path=/nlp_data/qinye/pretrains/RoBERTa_zh_L12_PyTorch
use_gpu=True
batch_size=10
lr=0.015
lr_decay=0.05
clip=5.0
warmup=False
end_epoch= 50
num_warmup_steps=1000
num_total_steps=10000

[Data]
create_if_exist=False
char_emb_path=/nlp_data/qinye/pretrains/word2vec/sgns.baidubaike.char
bichar_emb_path=/nlp_data/qinye/pretrains/word2vec/sgns.baidubaike.bichar
MAX_SENTENCE_LENGTH=1000
number_normalized=True
norm_word_emb=True
min_freq=1
word_emb_dim=300
biword_emb_dim=300
save_data_name=alphabet_data


[model_choice]
use_bigram=True
use_bert=True
model_type=lstm


[model_setting]
bilstm=True
hidden_dim=128
dropout=0.5
lstm_layer=1
num_layer=4


[model_test]
test_device_id=1
pred_out_path=/nlp_data/qinye/ner_task/pred
test_batch_size=10
